{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 1_hot_small_sahi_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parisasalma/FYP/blob/master/Copy_of_1_hot_small_sahi_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G_3BofAM_hwB",
        "colab_type": "code",
        "outputId": "ac870213-129c-42bf-98e3-3f557e60a61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.61)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.61 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.61)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.61->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.61->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O2za-YRPkhHD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.parsing import PorterStemmer\n",
        "from gensim.models import Word2Vec, Phrases, phrases, KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk import tokenize\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "from itertools import chain\n",
        "\n",
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import re\n",
        "import tarfile\n",
        "#os.environ['THEANO_FLAGS'] = \"device=cuda×\"  \n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from importlib import reload\n",
        "import sys\n",
        "reload(sys)\n",
        "#sys.setdefaultencoding('utf-8')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9VReTy5MR_UR",
        "colab_type": "code",
        "outputId": "73815ba4-eea2-49e5-c723-9961f44be206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FtJfcdhHWaiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import re\n",
        "import tarfile\n",
        "#os.environ['THEANO_FLAGS'] = \"device=cuda×\"  \n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import merge\n",
        "from keras.layers import Dense,  Dropout, RepeatVector\n",
        "from keras.layers import add,Input\n",
        "from keras.layers import recurrent\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWML1Rydq1e0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epTG7pFU6ndM",
        "colab_type": "code",
        "outputId": "00eed793-6382-4fdc-9b7f-0faa1245e4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "49OxJ1lBArMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##########################################\n",
        "\n",
        "Data Pre-Processing\n",
        "\n",
        "##########################################"
      ]
    },
    {
      "metadata": {
        "id": "ifFybxTEajdt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    # Return a list of words\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCGNE1-XwBV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file=open('/content/gdrive/My Drive/Colab_Notebooks/MCtest/mctest-master/data/MCTest/mc160.train.txt','r')\n",
        "#file = open(\"data/MCTest/mc160.train.txt\", \"r\") \n",
        "obj= (file.read())\n",
        "splitss1=(obj.split('***************************************************'))\n",
        "\n",
        "file2=open('/content/gdrive/My Drive/Colab_Notebooks/MCtest/mctest-master/data/MCTest/mc160.dev.txt','r')\n",
        "#file = open(\"data/MCTest/mc160.train.txt\", \"r\") \n",
        "obj= (file2.read())\n",
        "splitss2=(obj.split('***************************************************'))\n",
        "splitss=splitss1+splitss2\n",
        "#print (splitss)\n",
        "s1=len(splitss1)\n",
        "s2=len(splitss2)\n",
        "s=s1+s2-2\n",
        "df_dd1 = pd.DataFrame(index=range(s),\n",
        "                   columns=['story','q1','q2','q3','q4','A1','A2','A3','A4'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a20b80c6-6896-4aa7-b460-41e075f9b834",
        "id": "L18QLI9jwQZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "index_df=0\n",
        "for index2 in range (0,2):\n",
        "  if index2==0:\n",
        "    splitss=splitss1\n",
        "    print ('00000000000000000000000000000000000000000')\n",
        "  if (index2==1):\n",
        "    splitss=splitss2\n",
        "    print ('11111111111111111111111111111111111111111')\n",
        "  for i in range(1,len(splitss)):\n",
        "      a=(splitss[i].split('\\n')[1].strip())\n",
        "      title=a.replace('Story:','')\n",
        "      title=title.replace(' ','')\n",
        "      #df2['id'][index_df]=title\n",
        "\n",
        "      b=(splitss[i].split('\\n')[2].strip())\n",
        "      author=b.replace('Author:','')\n",
        "      author=author.replace(' ','')\n",
        "\n",
        "      #df2['author'][index_df]=author\n",
        "      c=(splitss[i].split('\\n')[3].strip())\n",
        "      work_time=c.replace('Work Time(s):','')\n",
        "      work_time=work_time.replace(' ','')\n",
        "\n",
        "      story=(splitss[i].split(':')[3].strip())\n",
        "      story=story.replace(work_time,'')\n",
        "      story=story.replace('1','')\n",
        "      story=story.replace('\\n',' ')\n",
        "      story=story.replace('\\t',' ')\n",
        "      df_dd1['story'][index_df]=story#word_tokenize(remove_punct(story).lower())\n",
        "\n",
        "      quest1=(splitss[i].split(':')[5].strip())\n",
        "      quest1=quest1.split(')')[0].strip()\n",
        "      quest1=quest1[:-1]\n",
        "      #print ('quest11111111111')\n",
        "      #print(quest1)\n",
        "      CA1=\" \"\n",
        "      CA2=\" \"\n",
        "      CA3=\" \"\n",
        "      CA4=\" \"\n",
        "      q1a1=(splitss[i].split(')')[2].strip())\n",
        "      q1a1=q1a1[:-1]\n",
        "      if ('*' in quest1):\n",
        "        quest1=quest1.replace('*','')\n",
        "        CA1=q1a1\n",
        "\n",
        "      q1a2=(splitss[i].split(')')[3].strip())\n",
        "      q1a2=q1a2[:-1]\n",
        "\n",
        "      if ('*' in q1a1):\n",
        "        q1a1=q1a1.replace('*','')\n",
        "        CA1=q1a2\n",
        "\n",
        "      q1a3=(splitss[i].split(')')[4].strip())\n",
        "      q1a3=q1a3[:-1]\n",
        "      if ('*' in q1a2):\n",
        "        q1a2=q1a2.replace('*','')\n",
        "        CA1=q1a3\n",
        "\n",
        "      q1a4=(splitss[i].split(')')[5].strip())\n",
        "      q1a4=q1a4.split('2')[0].strip()\n",
        "\n",
        "      if ('*' in q1a3):\n",
        "        q1a3=q1a3.replace('*','')\n",
        "        CA1=q1a4\n",
        "      quest1=quest1.replace('\\n',' ')\n",
        "      quest1=quest1.replace('\\t',' ')\n",
        "      df_dd1['q1'][index_df]=quest1#word_tokenize(remove_punct(quest1).lower())\n",
        "      #df2['q1'][index_df]=quest1  \n",
        "\n",
        "      l=[]\n",
        "      #print (CA1)\n",
        "      #print (q1a1)\n",
        "      #print (q1a2)\n",
        "      #print (q1a3)\n",
        "      #print (q1a4)\n",
        "      l.append(q1a1)\n",
        "      l.append(q1a2)\n",
        "      l.append(q1a3)\n",
        "      l.append(q1a4)\n",
        "      #df2['A1'][i-1]=l\n",
        "\n",
        "      quest2=(splitss[i].split(':')[7].strip())\n",
        "      quest2=quest2.split(')')[0].strip()\n",
        "      quest2=quest2[:-1]\n",
        "      #print ('quest22222222222')\n",
        "      #print(quest2)    \n",
        "      q2a1=(splitss[i].split(')')[6].strip())\n",
        "      q2a1=q2a1[:-1]\n",
        "\n",
        "\n",
        "      if ('*' in quest2):\n",
        "        quest2=quest2.replace('*','')\n",
        "        CA2=q2a1\n",
        "\n",
        "      q2a2=(splitss[i].split(')')[7].strip())\n",
        "      q2a2=q2a2[:-1]\n",
        "      if ('*' in q2a1):\n",
        "        q2a1=q2a1.replace('*','')\n",
        "        CA2=q2a2\n",
        "\n",
        "      q2a3=(splitss[i].split(')')[8].strip())\n",
        "      q2a3 = q2a3[:-1]\n",
        "\n",
        "      if ('*' in q2a2):\n",
        "        q2a2=q2a2.replace('*','')\n",
        "        CA2=q2a3\n",
        "\n",
        "\n",
        "      q2a4=(splitss[i].split(')')[9].strip())\n",
        "      q2a4=q2a4.split('3')[0].strip()\n",
        "\n",
        "\n",
        "      if ('*' in q2a3):\n",
        "        q2a3=q2a3.replace('*','')\n",
        "        CA2=q2a4\n",
        "      quest2=quest2.replace('\\n',' ')\n",
        "      quest2=quest2.replace('\\t',' ')\n",
        "      df_dd1['q2'][index_df]=quest2#word_tokenize(remove_punct(quest2).lower())\n",
        "      #df2['q2'][index_df]=quest2\n",
        "\n",
        "      l=[]\n",
        "      #print (CA2)\n",
        "      #print (q2a1)\n",
        "      #print (q2a2)\n",
        "      #print (q2a3)\n",
        "      #print (q2a4)\n",
        "      l.append(q2a1)\n",
        "      l.append(q2a2)\n",
        "      l.append(q2a3)\n",
        "      l.append(q2a4)\n",
        "      #df2['A2'][i-1]=l\n",
        "\n",
        "      quest3=(splitss[i].split(':')[9].strip())\n",
        "      quest3=quest3.split(')')[0].strip()\n",
        "      quest3=quest3[:-1]\n",
        "      #print ('quest33333333333')\n",
        "      #print(quest3)\n",
        "\n",
        "      q3a1=(splitss[i].split(')')[10].strip())\n",
        "      q3a1=q3a1[:-1]\n",
        "\n",
        "      if ('*' in quest3):\n",
        "        quest3=quest3.replace('*','')\n",
        "        #q3a1=q3a1.replace(' ','')\n",
        "        CA3=q3a1\n",
        "\n",
        "      q3a2=(splitss[i].split(')')[11].strip())\n",
        "      q3a2=q3a2[:-1]\n",
        "\n",
        "      if ('*' in q3a1):\n",
        "        q3a1=q3a1.replace('*','')\n",
        "        CA3=q3a2\n",
        "\n",
        "      q3a3=(splitss[i].split(')')[12].strip())\n",
        "      q3a3=q3a3[:-1]\n",
        "\n",
        "      if ('*' in q3a2):\n",
        "        q3a2=q3a2.replace('*','')\n",
        "        CA3=q3a3\n",
        "\n",
        "\n",
        "      q3a4=(splitss[i].split(')')[13].strip())\n",
        "      q3a4=q3a4.split('4')[0].strip()\n",
        "      #q3a4=q3a4.replace('4','')\n",
        "\n",
        "      if ('*' in q3a3):\n",
        "        q3a3=q3a3.replace('*','')\n",
        "        CA3=q3a4\n",
        "\n",
        "      #print (CA3)\n",
        "      #print (q3a1)\n",
        "      #print (q3a2)\n",
        "      #print (q3a3)\n",
        "      #print (q3a4)\n",
        "      quest3=quest3.replace('\\n',' ')\n",
        "      quest3=quest3.replace('\\t',' ')\n",
        "      df_dd1['q3'][index_df]=quest3#word_tokenize(remove_punct(quest3).lower())\n",
        "      #df2['q3'][index_df]=quest3  \n",
        "      l=[]\n",
        "      l.append(q3a1)\n",
        "      l.append(q3a2)\n",
        "      l.append(q3a3)\n",
        "      l.append(q3a4)\n",
        "      #df2['A3'][i-1]=l\n",
        "\n",
        "      quest4=(splitss[i].split(':')[11].strip())\n",
        "      quest4=quest4.split(')')[0].strip()\n",
        "      quest4=quest4[:-1]\n",
        "      #print ('quest44444444444444')\n",
        "      #print(quest4)\n",
        "\n",
        "      q4a1=(splitss[i].split(')')[14].strip())\n",
        "      q4a1=q4a1[:-1]\n",
        "\n",
        "      if ('*' in quest4):\n",
        "        quest4=quest4.replace('*','')\n",
        "        CA4=q4a1\n",
        "\n",
        "      q4a2=(splitss[i].split(')')[15].strip())\n",
        "      q4a2=q4a2[:-1]\n",
        "\n",
        "      if ('*' in q4a1):\n",
        "        q4a1=q4a1.replace('*','')\n",
        "        CA4=q4a2\n",
        "\n",
        "      q4a3=(splitss[i].split(')')[16].strip())\n",
        "      q4a3=q4a3[:-1]\n",
        "\n",
        "      if ('*' in q4a2):\n",
        "        q4a2=q4a2.replace('*','')\n",
        "        CA4=q4a3\n",
        "\n",
        "\n",
        "      q4a4=(splitss[i].split(')')[17].strip())\n",
        "      q4a4=q4a4.split('/n')[0].strip()\n",
        "      q4a4=q4a4.replace('/n','')\n",
        "\n",
        "      if ('*' in q4a3):\n",
        "        q4a3=q4a3.replace('*','')\n",
        "        CA4=q4a4\n",
        "\n",
        "      #print (CA4)\n",
        "      #print (q4a1)\n",
        "      #print (q4a2)\n",
        "      #print (q4a3)\n",
        "      #print (q4a4)\n",
        "      quest4=quest4.replace('\\n',' ')\n",
        "      quest4=quest4.replace('\\t',' ')\n",
        "      df_dd1['q4'][index_df]=quest4#word_tokenize(remove_punct(quest4).lower())\n",
        "      l=[]\n",
        "      l.append(q4a1)\n",
        "      l.append(q4a2)\n",
        "      l.append(q4a3)\n",
        "      l.append(q4a4)\n",
        "      #df2['A4'][i-1]=l\n",
        "\n",
        "      l=[]\n",
        "      l.append(CA1)\n",
        "      l.append(CA2)\n",
        "      l.append(CA3)\n",
        "      l.append(CA4)\n",
        "      #df2['A1'][index_df]=CA1\n",
        "      #df2['A2'][index_df]=CA2\n",
        "      #df2['A3'][index_df]=CA3\n",
        "      #df2['A4'][index_df]=CA4\n",
        "      \n",
        "      CA1=CA1.replace('\\n',' ')\n",
        "      CA1=CA1.replace('\\t',' ')\n",
        "      #CA1=word_tokenize(remove_punct(CA1).lower())\n",
        "      df_dd1['A1'][index_df]=CA1\n",
        "      CA2=CA2.replace('\\n',' ')\n",
        "      CA2=CA2.replace('\\t',' ')\n",
        "      #CA2=word_tokenize(remove_punct(CA2).lower())\n",
        "      df_dd1['A2'][index_df]=CA2\n",
        "      CA3=CA3.replace('\\n',' ')\n",
        "      CA3=CA3.replace('\\t',' ')\n",
        "      #CA3=word_tokenize(remove_punct(CA3).lower())\n",
        "      df_dd1['A3'][index_df]=CA3\n",
        "      CA4=CA4.replace('\\n',' ')\n",
        "      CA4=CA4.replace('\\t',' ')\n",
        "      #CA4=word_tokenize(remove_punct(CA4).lower())\n",
        "      df_dd1['A4'][index_df]=CA4\n",
        "      #df2['CA'][i-1]=l\n",
        "      #print ('newwwwwwwwwwwww')\n",
        "      #print ('story ',i)\n",
        "      #print (story)\n",
        "      index_df=index_df+1\n",
        "      #print (index_df)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00000000000000000000000000000000000000000\n",
            "11111111111111111111111111111111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjHa0XkXys4o",
        "colab_type": "code",
        "outputId": "e6c1e51d-4bf2-4b53-e05e-d05a7061c6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "file2=open('/content/gdrive/My Drive/Colab_Notebooks/MCtest/mctest-master/data/MCTest/mc160.test.txt','r')\n",
        "#file = open(\"data/MCTest/mc160.train.txt\", \"r\") \n",
        "obj= (file2.read())\n",
        "splitss2=(obj.split('***************************************************'))\n",
        "s2=len(splitss2)\n",
        "print (s2)\n",
        "df_dd2 = pd.DataFrame(index=range(s2-1),\n",
        "                   columns=['story','q1','q2','q3','q4','A1','A2','A3','A4'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "11846ef7-1fde-4f2b-c8d5-324be66cb144",
        "id": "9sV2FfGzy5kL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "index_df=0\n",
        "for index2 in range (0,1):\n",
        "  if index2==0:\n",
        "    splitss=splitss2\n",
        "    print ('00000000000000000000000000000000000000000')\n",
        "  if (index2==1):\n",
        "    splitss=splitss2\n",
        "    print ('11111111111111111111111111111111111111111')\n",
        "  for i in range(1,len(splitss)):\n",
        "      a=(splitss[i].split('\\n')[1].strip())\n",
        "      title=a.replace('Story:','')\n",
        "      title=title.replace(' ','')\n",
        "      #dfA['id'][index_df]=title\n",
        "\n",
        "      b=(splitss[i].split('\\n')[2].strip())\n",
        "      author=b.replace('Author:','')\n",
        "      author=author.replace(' ','')\n",
        "\n",
        "      #dfA['author'][index_df]=author\n",
        "      c=(splitss[i].split('\\n')[3].strip())\n",
        "      work_time=c.replace('Work Time(s):','')\n",
        "      work_time=work_time.replace(' ','')\n",
        "\n",
        "      story=(splitss[i].split(':')[3].strip())\n",
        "      story=story.replace(work_time,'')\n",
        "      story=story.replace('1','')\n",
        "      story=story.replace('\\n',' ')\n",
        "      story=story.replace('\\t',' ')\n",
        "      df_dd2['story'][index_df]=story#word_tokenize(remove_punct(story).lower())\n",
        "\n",
        "      quest1=(splitss[i].split(':')[5].strip())\n",
        "      quest1=quest1.split(')')[0].strip()\n",
        "      quest1=quest1[:-1]\n",
        "      #print ('quest11111111111')\n",
        "      #print(quest1)\n",
        "      CA1=\" \"\n",
        "      CA2=\" \"\n",
        "      CA3=\" \"\n",
        "      CA4=\" \"\n",
        "      q1a1=(splitss[i].split(')')[2].strip())\n",
        "      q1a1=q1a1[:-1]\n",
        "      if ('*' in quest1):\n",
        "        quest1=quest1.replace('*','')\n",
        "        CA1=q1a1\n",
        "\n",
        "      q1a2=(splitss[i].split(')')[3].strip())\n",
        "      q1a2=q1a2[:-1]\n",
        "\n",
        "      if ('*' in q1a1):\n",
        "        q1a1=q1a1.replace('*','')\n",
        "        CA1=q1a2\n",
        "\n",
        "      q1a3=(splitss[i].split(')')[4].strip())\n",
        "      q1a3=q1a3[:-1]\n",
        "      if ('*' in q1a2):\n",
        "        q1a2=q1a2.replace('*','')\n",
        "        CA1=q1a3\n",
        "\n",
        "\n",
        "      q1a4=(splitss[i].split(')')[5].strip())\n",
        "      q1a4=q1a4.split('2')[0].strip()\n",
        "\n",
        "      if ('*' in q1a3):\n",
        "        q1a3=q1a3.replace('*','')\n",
        "        CA1=q1a4\n",
        "      quest1=quest1.replace('\\n',' ')\n",
        "      quest1=quest1.replace('\\t',' ')\n",
        "      df_dd2['q1'][index_df]=quest1#word_tokenize(remove_punct(quest1).lower())\n",
        "      #df2['q1'][index_df]=quest1  \n",
        "\n",
        "      l=[]\n",
        "      #print (CA1)\n",
        "      #print (q1a1)\n",
        "      #print (q1a2)\n",
        "      #print (q1a3)\n",
        "      #print (q1a4)\n",
        "      l.append(q1a1)\n",
        "      l.append(q1a2)\n",
        "      l.append(q1a3)\n",
        "      l.append(q1a4)\n",
        "      #df2['A1'][i-1]=l\n",
        "\n",
        "      quest2=(splitss[i].split(':')[7].strip())\n",
        "      quest2=quest2.split(')')[0].strip()\n",
        "      quest2=quest2[:-1]\n",
        "      #print ('quest22222222222')\n",
        "      #print(quest2)    \n",
        "      q2a1=(splitss[i].split(')')[6].strip())\n",
        "      q2a1=q2a1[:-1]\n",
        "\n",
        "\n",
        "      if ('*' in quest2):\n",
        "        quest2=quest2.replace('*','')\n",
        "        CA2=q2a1\n",
        "\n",
        "      q2a2=(splitss[i].split(')')[7].strip())\n",
        "      q2a2=q2a2[:-1]\n",
        "      if ('*' in q2a1):\n",
        "        q2a1=q2a1.replace('*','')\n",
        "        CA2=q2a2\n",
        "\n",
        "      q2a3=(splitss[i].split(')')[8].strip())\n",
        "      q2a3 = q2a3[:-1]\n",
        "\n",
        "      if ('*' in q2a2):\n",
        "        q2a2=q2a2.replace('*','')\n",
        "        CA2=q2a3\n",
        "\n",
        "\n",
        "      q2a4=(splitss[i].split(')')[9].strip())\n",
        "      q2a4=q2a4.split('3')[0].strip()\n",
        "\n",
        "\n",
        "      if ('*' in q2a3):\n",
        "        q2a3=q2a3.replace('*','')\n",
        "        CA2=q2a4\n",
        "      quest2=quest2.replace('\\n',' ')\n",
        "      quest2=quest2.replace('\\t',' ')\n",
        "      df_dd2['q2'][index_df]=quest2#word_tokenize(remove_punct(quest2).lower())\n",
        "      #df2['q2'][index_df]=quest2\n",
        "\n",
        "      l=[]\n",
        "      #print (CA2)\n",
        "      #print (q2a1)\n",
        "      #print (q2a2)\n",
        "      #print (q2a3)\n",
        "      #print (q2a4)\n",
        "      l.append(q2a1)\n",
        "      l.append(q2a2)\n",
        "      l.append(q2a3)\n",
        "      l.append(q2a4)\n",
        "      #df2['A2'][i-1]=l\n",
        "\n",
        "      quest3=(splitss[i].split(':')[9].strip())\n",
        "      quest3=quest3.split(')')[0].strip()\n",
        "      quest3=quest3[:-1]\n",
        "      #print ('quest33333333333')\n",
        "      #print(quest3)\n",
        "\n",
        "      q3a1=(splitss[i].split(')')[10].strip())\n",
        "      q3a1=q3a1[:-1]\n",
        "\n",
        "      if ('*' in quest3):\n",
        "        quest3=quest3.replace('*','')\n",
        "        #q3a1=q3a1.replace(' ','')\n",
        "        CA3=q3a1\n",
        "\n",
        "      q3a2=(splitss[i].split(')')[11].strip())\n",
        "      q3a2=q3a2[:-1]\n",
        "\n",
        "      if ('*' in q3a1):\n",
        "        q3a1=q3a1.replace('*','')\n",
        "        CA3=q3a2\n",
        "\n",
        "      q3a3=(splitss[i].split(')')[12].strip())\n",
        "      q3a3=q3a3[:-1]\n",
        "\n",
        "      if ('*' in q3a2):\n",
        "        q3a2=q3a2.replace('*','')\n",
        "        CA3=q3a3\n",
        "\n",
        "\n",
        "      q3a4=(splitss[i].split(')')[13].strip())\n",
        "      q3a4=q3a4.split('4')[0].strip()\n",
        "      #q3a4=q3a4.replace('4','')\n",
        "\n",
        "      if ('*' in q3a3):\n",
        "        q3a3=q3a3.replace('*','')\n",
        "        CA3=q3a4\n",
        "\n",
        "      #print (CA3)\n",
        "      #print (q3a1)\n",
        "      #print (q3a2)\n",
        "      #print (q3a3)\n",
        "      #print (q3a4)\n",
        "      quest3=quest3.replace('\\n',' ')\n",
        "      quest3=quest3.replace('\\t',' ')\n",
        "      df_dd2['q3'][index_df]=quest3#word_tokenize(remove_punct(quest3).lower())\n",
        "      #df2['q3'][index_df]=quest3  \n",
        "      l=[]\n",
        "      l.append(q3a1)\n",
        "      l.append(q3a2)\n",
        "      l.append(q3a3)\n",
        "      l.append(q3a4)\n",
        "      #df2['A3'][i-1]=l\n",
        "\n",
        "      quest4=(splitss[i].split(':')[11].strip())\n",
        "      quest4=quest4.split(')')[0].strip()\n",
        "      quest4=quest4[:-1]\n",
        "      #print ('quest44444444444444')\n",
        "      #print(quest4)\n",
        "\n",
        "      q4a1=(splitss[i].split(')')[14].strip())\n",
        "      q4a1=q4a1[:-1]\n",
        "\n",
        "      if ('*' in quest4):\n",
        "        quest4=quest4.replace('*','')\n",
        "        CA4=q4a1\n",
        "\n",
        "      q4a2=(splitss[i].split(')')[15].strip())\n",
        "      q4a2=q4a2[:-1]\n",
        "\n",
        "      if ('*' in q4a1):\n",
        "        q4a1=q4a1.replace('*','')\n",
        "        CA4=q4a2\n",
        "\n",
        "      q4a3=(splitss[i].split(')')[16].strip())\n",
        "      q4a3=q4a3[:-1]\n",
        "\n",
        "      if ('*' in q4a2):\n",
        "        q4a2=q4a2.replace('*','')\n",
        "        CA4=q4a3\n",
        "\n",
        "\n",
        "      q4a4=(splitss[i].split(')')[17].strip())\n",
        "      q4a4=q4a4.split('/n')[0].strip()\n",
        "      q4a4=q4a4.replace('/n','')\n",
        "\n",
        "      if ('*' in q4a3):\n",
        "        q4a3=q4a3.replace('*','')\n",
        "        CA4=q4a4\n",
        "\n",
        "      #print (CA4)\n",
        "      #print (q4a1)\n",
        "      #print (q4a2)\n",
        "      #print (q4a3)\n",
        "      #print (q4a4)\n",
        "      quest4=quest4.replace('\\n',' ')\n",
        "      quest4=quest4.replace('\\t',' ')\n",
        "      df_dd2['q4'][index_df]=quest4#word_tokenize(remove_punct(quest4).lower())\n",
        "      l=[]\n",
        "      l.append(q4a1)\n",
        "      l.append(q4a2)\n",
        "      l.append(q4a3)\n",
        "      l.append(q4a4)\n",
        "      \n",
        "\n",
        "      l=[]\n",
        "      l.append(CA1)\n",
        "      l.append(CA2)\n",
        "      l.append(CA3)\n",
        "      l.append(CA4)\n",
        "      CA1=CA1.replace('\\n',' ')\n",
        "      CA1=CA1.replace('\\t',' ')\n",
        "      #CA1=word_tokenize(remove_punct(CA1).lower())\n",
        "      df_dd2['A1'][index_df]=CA1\n",
        "      CA2=CA2.replace('\\n',' ')\n",
        "      CA2=CA2.replace('\\t',' ')\n",
        "      #CA2=word_tokenize(remove_punct(CA2).lower())\n",
        "      df_dd2['A2'][index_df]=CA2\n",
        "      CA3=CA3.replace('\\n',' ')\n",
        "      CA3=CA3.replace('\\t',' ')\n",
        "      #CA3=word_tokenize(remove_punct(CA3).lower())\n",
        "      df_dd2['A3'][index_df]=CA3\n",
        "      CA4=CA4.replace('\\n',' ')\n",
        "      CA4=CA4.replace('\\t',' ')\n",
        "      #CA4=word_tokenize(remove_punct(CA4).lower())\n",
        "      df_dd2['A4'][index_df]=CA4\n",
        "      \n",
        "      #print ('newwwwwwwwwwwww')\n",
        "      #print ('story ',i)\n",
        "      #print (story)\n",
        "      index_df=index_df+1\n",
        "      #print (index_df)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00000000000000000000000000000000000000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ndbn2XFnytMF",
        "colab_type": "code",
        "outputId": "32491898-bb4e-444e-fc61-d0f3e069bdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_dd1_ = pd.DataFrame(index=range(len(df_dd1)*4),\n",
        "                   columns=[ 'story','q','A'])\n",
        "print (len(df_dd1_))\n",
        "indx=0\n",
        "for i in range(0,len(df_dd1)):\n",
        "    df_dd1_['story'][indx]=df_dd1['story'][i]\n",
        "    df_dd1_['q'][indx]=df_dd1['q1'][i]\n",
        "    df_dd1_['A'][indx]=df_dd1['A1'][i]\n",
        "    indx=indx+1\n",
        "    df_dd1_['story'][indx]=df_dd1['story'][i]\n",
        "    df_dd1_['q'][indx]=df_dd1['q2'][i]\n",
        "    df_dd1_['A'][indx]=df_dd1['A2'][i]\n",
        "    indx=indx+1\n",
        "    df_dd1_['story'][indx]=df_dd1['story'][i]\n",
        "    df_dd1_['q'][indx]=df_dd1['q3'][i]\n",
        "    df_dd1_['A'][indx]=df_dd1['A3'][i]\n",
        "    indx=indx+1\n",
        "    df_dd1_['story'][indx]=df_dd1['story'][i]\n",
        "    df_dd1_['q'][indx]=df_dd1['q4'][i]\n",
        "    df_dd1_['A'][indx]=df_dd1['A4'][i]\n",
        "    indx=indx+1\n",
        "    #print (indx)\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0he-wgjm1KVY",
        "colab_type": "code",
        "outputId": "0e03b2de-2680-4d61-ac44-d575f7bf37e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_dd2_ = pd.DataFrame(index=range(len(df_dd2)*4),\n",
        "                   columns=[ 'story','q','A'])\n",
        "print (len(df_dd2_))\n",
        "indx=0\n",
        "for i in range(0,len(df_dd2)):\n",
        "    df_dd2_['story'][indx]=df_dd2['story'][i]\n",
        "    df_dd2_['q'][indx]=df_dd2['q1'][i]\n",
        "    df_dd2_['A'][indx]=df_dd2['A1'][i]\n",
        "    indx=indx+1\n",
        "    df_dd2_['story'][indx]=df_dd1['story'][i]\n",
        "    df_dd2_['q'][indx]=df_dd2['q2'][i]\n",
        "    df_dd2_['A'][indx]=df_dd2['A2'][i]\n",
        "    indx=indx+1\n",
        "    df_dd2_['story'][indx]=df_dd2['story'][i]\n",
        "    df_dd2_['q'][indx]=df_dd2['q3'][i]\n",
        "    df_dd2_['A'][indx]=df_dd2['A3'][i]\n",
        "    indx=indx+1\n",
        "    df_dd2_['story'][indx]=df_dd2['story'][i]\n",
        "    df_dd2_['q'][indx]=df_dd2['q4'][i]\n",
        "    df_dd2_['A'][indx]=df_dd2['A4'][i]\n",
        "    indx=indx+1\n",
        "    #print (indx)\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2kK52UEiwYdd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 500\n",
        "MAX_NB_WORDS = 200000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "\n",
        "num_lstm = np.random.randint(175, 275)\n",
        "num_dense = np.random.randint(100, 150)\n",
        "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
        "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pt_cy31VT8n-",
        "colab_type": "code",
        "outputId": "1796b3d0-094a-42f4-805c-0db86a570db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "texts_1=[]\n",
        "for i in df_dd1_['story']:\n",
        "  texts_1.append(i)\n",
        "texts_2=[]\n",
        "for i in df_dd1_['q']:\n",
        "  texts_2.append(i)\n",
        "\n",
        "\n",
        "label=[]\n",
        "for i in df_dd1_['A']:\n",
        "  label.append(i)  \n",
        "\n",
        "print('Found %s texts in train.csv' % len(texts_1))\n",
        "\n",
        "\n",
        "test_texts_1 = []\n",
        "for i in df_dd2_['story']:\n",
        "  test_texts_1.append(i)\n",
        "test_texts_2 = []\n",
        "for i in df_dd2_['q']:\n",
        "  test_texts_2.append(i)\n",
        "\n",
        "print('Found %s texts in test.csv' % len(test_texts_1))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 texts in train.csv\n",
            "Found 240 texts in test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KaX4r4V0_7hB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##################################\n",
        "\n",
        "Tokenizer\n",
        "\n",
        "##################################"
      ]
    },
    {
      "metadata": {
        "id": "5dix_aK9bmXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(np.array(texts_1 + texts_2 + test_texts_1 + test_texts_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPPTgjR9bmiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
        "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
        "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
        "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5e9ReR_lcCp",
        "colab_type": "code",
        "outputId": "e54f1c4a-2c2b-4ea6-c259-18efac2050dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "label_1 = tokenizer.texts_to_sequences(label)\n",
        "print (len(label_1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JHJlqeL45V9C",
        "colab_type": "code",
        "outputId": "a29d2a64-572e-47a4-cf1a-dabd0e20db24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print('Shape of data tensor:', data_1.shape)\n",
        "\n",
        "\n",
        "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2836 unique tokens\n",
            "Shape of data tensor: (400, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N033bWAg_dV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#################################\n",
        "\n",
        "Word2vector\n",
        "\n",
        "#################################"
      ]
    },
    {
      "metadata": {
        "id": "oryfQ3ZNYh0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "token_data=[]\n",
        "\n",
        "for i in texts_1:\n",
        "  token_data.append(word_tokenize(i.lower()))\n",
        "for i in texts_2:\n",
        "  token_data.append(word_tokenize(i.lower()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BiXczwjacWOP",
        "colab_type": "code",
        "outputId": "a2febfa0-0353-48af-cb01-70349236ede6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## index word vectors\n",
        "########################################\n",
        "print('Indexing word vectors')\n",
        "\n",
        "word2vec = Word2Vec(size=300, min_count=1, iter=10)\n",
        "word2vec.build_vocab(np.array(token_data))\n",
        "word2vec.intersect_word2vec_format(\"/content/gdrive/My Drive/Colab_Notebooks/GoogleNews-vectors-negative300.bin.gz\",binary=True, lockf=1.0)\n",
        "word2vec.train(np.array(token_data),total_examples=len(token_data), epochs=10)\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(672899, 977270)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "bh2FE1SJJxUq",
        "colab_type": "code",
        "outputId": "7366bcee-3242-4685-8afe-30d570c9146c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (np.shape(data_2))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvcNcERgMoCT",
        "colab_type": "code",
        "outputId": "292f14ad-85fb-4432-a8ee-113dc154de0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "########### 1 hot encoding of label ###########\n",
        "#print (len(label_1))\n",
        "#print(label_1)\n",
        "siz=len(word_index.keys())\n",
        "#print (siz)\n",
        "ans=np.zeros((400,siz+1))\n",
        "print (np.shape(ans))\n",
        "for ind,i in enumerate(label_1):\n",
        "\n",
        "  for j in i:\n",
        "    \n",
        "    ans[ind,j]=1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400, 2837)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V6EBsNxy7NLm",
        "colab_type": "code",
        "outputId": "55251324-cbfe-479a-a79b-815ccfc2ac80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (len(ans[0]))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RRQnJIew_GUm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Getting Embedding Weights"
      ]
    },
    {
      "metadata": {
        "id": "_76SHeF9cnwU",
        "colab_type": "code",
        "outputId": "b122b4c4-dbe4-4dc9-e4f0-3dd4dff394a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## prepare embeddings\n",
        "########################################\n",
        "print('Preparing embedding matrix')\n",
        "\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "voc=word2vec.wv.vocab\n",
        "for word, i in word_index.items():\n",
        "    if word in voc:\n",
        "        embedding_matrix[i] = word2vec[word]\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "Null word embeddings: 685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "m1gAoywT7y9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VS3ogHj97zD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print (len(labels_train[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bsjaSm9l_MRu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model"
      ]
    },
    {
      "metadata": {
        "id": "XNBz1993-Lc9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## define the model structure\n",
        "########################################\n",
        "embedding_layer = Embedding(nb_words,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False)\n",
        "\n",
        "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
        "\n",
        "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
        "x1 = lstm_layer(embedded_sequences_1)\n",
        "\n",
        "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
        "y1 = lstm_layer(embedded_sequences_2)\n",
        "\n",
        "merged = concatenate([x1, y1])\n",
        "preds = Dense(2837)(merged)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hN56jUuydB-V",
        "colab_type": "code",
        "outputId": "432c2480-c232-4317-ef10-139a65944188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## train the model\n",
        "########################################\n",
        "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
        "        outputs=preds)\n",
        "model.compile(loss='mse',\n",
        "        optimizer='nadam',#'nadam',#'rmsprop',#'nadam',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "#print(STAMP)\n",
        "\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
        "#bst_model_path = STAMP + '.h5'\n",
        "#model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 500, 300)     851100      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 198)          395208      embedding_1[0][0]                \n",
            "                                                                 embedding_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 396)          0           lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2837)         1126289     concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,372,597\n",
            "Trainable params: 1,521,497\n",
            "Non-trainable params: 851,100\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0HcsJg9wdGNL",
        "colab_type": "code",
        "outputId": "f41ef147-29fb-4684-d8de-3beddaa9b96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "cell_type": "code",
      "source": [
        "hist = model.fit(([data_1[:100,:], data_2[:100,:]]), ans[:100,:], \\\n",
        "                 validation_split=0.1,\\\n",
        "        epochs=20, shuffle=True ) #, callbacks=[early_stopping, model_checkpoint])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "90/90 [==============================] - 8s 86ms/step - loss: 7.0106e-04 - acc: 0.6222 - val_loss: 0.0016 - val_acc: 0.3000\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 6.7969e-04 - acc: 0.6111 - val_loss: 0.0016 - val_acc: 0.3000\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 6.5474e-04 - acc: 0.5778 - val_loss: 0.0016 - val_acc: 0.3000\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 6.3303e-04 - acc: 0.5889 - val_loss: 0.0016 - val_acc: 0.3000\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 6.1921e-04 - acc: 0.6667 - val_loss: 0.0016 - val_acc: 0.4000\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 5.8575e-04 - acc: 0.6222 - val_loss: 0.0016 - val_acc: 0.4000\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 5.7191e-04 - acc: 0.6556 - val_loss: 0.0016 - val_acc: 0.4000\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 5.5354e-04 - acc: 0.6889 - val_loss: 0.0016 - val_acc: 0.4000\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 5.3422e-04 - acc: 0.6556 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 5.1996e-04 - acc: 0.6222 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 5.0300e-04 - acc: 0.6778 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 4.8309e-04 - acc: 0.6667 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 13/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 4.6665e-04 - acc: 0.6222 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 14/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 4.4715e-04 - acc: 0.6667 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 15/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 4.3246e-04 - acc: 0.6667 - val_loss: 0.0017 - val_acc: 0.4000\n",
            "Epoch 16/20\n",
            "90/90 [==============================] - 9s 96ms/step - loss: 4.2703e-04 - acc: 0.6667 - val_loss: 0.0017 - val_acc: 0.3000\n",
            "Epoch 17/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 4.0331e-04 - acc: 0.7667 - val_loss: 0.0018 - val_acc: 0.4000\n",
            "Epoch 18/20\n",
            "90/90 [==============================] - 9s 97ms/step - loss: 3.9131e-04 - acc: 0.7000 - val_loss: 0.0018 - val_acc: 0.4000\n",
            "Epoch 19/20\n",
            "90/90 [==============================] - 9s 99ms/step - loss: 3.8131e-04 - acc: 0.7111 - val_loss: 0.0018 - val_acc: 0.4000\n",
            "Epoch 20/20\n",
            "90/90 [==============================] - 9s 101ms/step - loss: 3.6625e-04 - acc: 0.7000 - val_loss: 0.0018 - val_acc: 0.3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "90Izi2QPCrw_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "############################################\n",
        "\n",
        "Prediction\n",
        "\n",
        "############################################"
      ]
    },
    {
      "metadata": {
        "id": "kxQ_rg6binbU",
        "colab_type": "code",
        "outputId": "e5f8c661-a9d3-4551-c089-63baf86802ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict([data_1[0:100,:], data_2[0:100,:]], verbose=1 )"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 2s 21ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xId5NdVNi4zN",
        "colab_type": "code",
        "outputId": "a2decb14-4e85-4357-8b6c-24899244048b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1759
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(0,len(preds)-1):\n",
        "  ind=np.argmax(preds[i])\n",
        "  for key in word_index:  # dict keys\n",
        "    if (word_index[key]==ind):\n",
        "       print (key, word_index[key])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "window 364\n",
            "toilet 1193\n",
            "dolly 1863\n",
            "jim 224\n",
            "saturday 742\n",
            "and 3\n",
            "and 3\n",
            "a 4\n",
            "a 4\n",
            "kate 870\n",
            "a 4\n",
            "kate 870\n",
            "trying 381\n",
            "because 68\n",
            "a 4\n",
            "dinner 227\n",
            "alex 445\n",
            "james 111\n",
            "to 2\n",
            "wanted 71\n",
            "stephan 1377\n",
            "couldn't 319\n",
            "sunny 497\n",
            "restrooms 1893\n",
            "was 5\n",
            "bill 744\n",
            "because 68\n",
            "friendly 614\n",
            "wet 1087\n",
            "felix 1312\n",
            "trash 659\n",
            "river 283\n",
            "baseball 179\n",
            "cap 1904\n",
            "stamps 1903\n",
            "things 181\n",
            "knight 954\n",
            "king 838\n",
            "king 838\n",
            "king 838\n",
            "it 14\n",
            "bananas 2253\n",
            "bananas 2253\n",
            "bananas 2253\n",
            "happy 81\n",
            "greg 1387\n",
            "story 209\n",
            "money 202\n",
            "hours 746\n",
            "make 145\n",
            "baseball 179\n",
            "tommy 261\n",
            "father 378\n",
            "the 1\n",
            "the 1\n",
            "an 128\n",
            "brown 220\n",
            "with 17\n",
            "the 1\n",
            "thomas 349\n",
            "and 3\n",
            "spike 221\n",
            "and 3\n",
            "jessica 726\n",
            "tim 222\n",
            "jim 224\n",
            "20 819\n",
            "greg 1387\n",
            "racing 1596\n",
            "a 4\n",
            "to 2\n",
            "hang 1398\n",
            "milk 485\n",
            "goats 1402\n",
            "and 3\n",
            "and 3\n",
            "to 2\n",
            "and 3\n",
            "book 963\n",
            "butterflies 370\n",
            "a 4\n",
            "play 47\n",
            "john 137\n",
            "john 137\n",
            "basketball 362\n",
            "a 4\n",
            "a 4\n",
            "george 326\n",
            "and 3\n",
            "and 3\n",
            "fries 1093\n",
            "and 3\n",
            "find 198\n",
            "a 4\n",
            "trash 659\n",
            "father 378\n",
            "she 10\n",
            "a 4\n",
            "and 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q8OgFFiOSyAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "preds = model.predict([test_data_1, test_data_2], verbose=1 )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9wqjmc8Smr9",
        "colab_type": "code",
        "outputId": "9d68283e-7b2a-447c-8bd5-c48beeb3f82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BC4YaITYGaba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hy-YSLxBHRGg",
        "colab_type": "code",
        "outputId": "cc9d7dfe-b6c2-4120-eed3-54954d783b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nmwnl9rtRDuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "json_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9NBrRKIJjau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnEvzMAcOXOO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QnsY-ExXjW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "question=['who did tom call']\n",
        "question=tokenizer.texts_to_sequences(question)\n",
        "question = pad_sequences(question, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "question=np.array(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGRx79P5C2rL",
        "colab_type": "code",
        "outputId": "7fbc1123-8b03-4f54-f6b1-d51d70ce92f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "answer = model.predict([data_1[0:1,:],question], verbose=1 )\n",
        "for i in range(len(answer)):\n",
        "  ind=np.argmax(answer[i])\n",
        "  for key in word_index:  # dict keys\n",
        "    if (word_index[key]==ind):\n",
        "       print (key, word_index[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 0s 118ms/step\n",
            "jim 224\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}